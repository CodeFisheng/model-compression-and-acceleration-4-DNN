* [FEED: Feature-level Ensemble Effect for knowledge Distillation](https://openreview.net/forum?id=BJxYEsAqY7)
* [Like What You Like: Knowledge Distill via Neuron Selectivity Transfer](https://openreview.net/forum?id=rJf0BjAqYX
)
* [A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation](https://openreview.net/forum?id=r14EOsCqKX)
* [KNOWLEDGE DISTILL VIA LEARNING NEURON MANIFOLD](https://openreview.net/forum?id=SJlYcoCcKX)
* [Multilingual Neural Machine Translation with Knowledge Distillation ](https://openreview.net/forum?id=S1gUsoR9YX)
* [Knowledge Distillation from Few Samples](https://openreview.net/forum?id=HkgDTiCctQ)
* [Exploration by random distillation](https://openreview.net/forum?id=H1lJJnR5Ym)
* [Learning Global Additive Explanations for Neural Nets Using Model Distillation](https://openreview.net/forum?id=SJl8J30qFX)
