# csdnWriter
* [paper summary](https://www.jianshu.com/p/e73851f32c9f)  
* [csdn writer](https://blog.csdn.net/yingpeng_zhong?t=1)  
* [综述作者](https://blog.csdn.net/wspba)  
* [压缩神经网络实验及简直策略等](https://blog.csdn.net/jason19966)  
* [several paper](https://blog.csdn.net/cookie_234?t=1)  
* [地大大](https://blog.csdn.net/liujianlin01)  
# articles
* [合集](https://www.jiqizhixin.com/articles/2018-06-01-11)  
* [模型压缩论文及其相关](https://www.jishux.com/p/4007c8b22f2c7083)  
* [神经网络模型压缩与加速的常见方法](https://blog.csdn.net/LiJiancheng0614/article/details/79478792?utm_source=blogxgwz1)  
* [prune和网络剪枝](https://blog.csdn.net/SIGAI_CSDN/article/details/80803956?utm_source=blogxgwz8)  
* [github summary](https://github.com/Ewenwan/MVision/tree/master/CNN/Deep_Compression/quantization)
# Pruning
* [Soft Weight-Sharing for Neural Network Compression ](http://cn.arxiv.org/abs/1702.04008)
* [Channel Pruning for Accelerating Very Deep Neural Networks](https://arxiv.org/abs/1707.06168)
* [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)
* [MODEL COMPRESSION WITH GENERATIVE ADVERSARIAL NETWORKS](https://openreview.net/forum?id=Byxz4n09tQ)
* [Dynamic Network Surgery for Efficient DNNs](https://arxiv.org/abs/1608.04493)
* [To prune, or not to prune: exploring the efficacy of pruning for model compression](http://cn.arxiv.org/abs/1710.01878)
* [Learning both Weights and Connections for Efficient Neural Networks](http://cn.arxiv.org/abs/1506.02626)
# quantization
## binarization
* [binary neural network](https://arxiv.org/abs/1602.02830)
* [XNOR-net](https://arxiv.org/abs/1603.05279)
* [HORQ-net](https://arxiv.org/abs/1708.08687)
* [ternary neural network](https://arxiv.org/abs/1705.01462)
* [DeRefa-net](https://arxiv.org/abs/1606.06160)
* [YodaNN](https://arxiv.org/abs/1606.05487)
* [BinaryRelax- A Relaxation Approach For Training Deep Neural Networks With Quantized Weights](http://cn.arxiv.org/abs/1801.06313)
## fixed-point quantization
* [fixed-point factorized network](https://arxiv.org/abs/1611.01972)
## quantization method
* [incremental network quantization](https://arxiv.org/abs/1702.03044)
* [Hashed-Net](https://arxiv.org/abs/1702.00758)
* [Quantized Convolutional Neural Networks for Mobile Devices](https://arxiv.org/abs/1512.06473)
## quantization article
* [summary](https://blog.csdn.net/u012101561/article/details/80868352?utm_source=blogxgwz34)
# Net
* [GoogLeNet](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)
* [SqueezeNet](https://arxiv.org/abs/1602.07360)
* [MobileNets](https://arxiv.org/abs/1704.04861)
* [ShuffleNet](https://arxiv.org/abs/1707.01083)
