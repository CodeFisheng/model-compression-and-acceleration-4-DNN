# model COMPRESSION
* [Towards Image Understanding from Deep Compression without Decoding](http://cn.arxiv.org/abs/1803.06131)
* [Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training](https://openreview.net/forum?id=SkhQHMW0W)
* [N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning](https://openreview.net/forum?id=B1hcZZ-AW)
# quantization
* [Alternating Multi-bit Quantization for Recurrent Neural Networks](http://cn.arxiv.org/abs/1802.00150)
* [Adaptive Quantization for Deep Neural Network](https://arxiv.org/abs/1712.01048)
* [Variational Network Quantization](https://openreview.net/forum?id=ry-TW-WAb)
* [Model compression via distillation and quantization](https://openreview.net/forum?id=S1XolQbRW)
* [Loss-aware Weight Quantization of Deep Networks](https://openreview.net/forum?id=BkrSv0lA-)
# distillation
* [Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy](http://cn.arxiv.org/abs/1711.05852)
