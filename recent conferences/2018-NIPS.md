# compression
* [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://arxiv.org/abs/1802.04977)
* [Communication Compression for Decentralized Training](https://arxiv.org/abs/1803.06443)
  * 似乎发表了一个既能解决带宽要求，也能实现去中心化的系统。
  * 提出一个量化的且去中心化的训练过程，分为如下两个阶段
    * extrapolation compression
    * difference compression
  * highlight
    * quantized
    * decentralized
# acceleration
* [Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution]
  * 针对高维卷积耗时的问题，提出Acceleration Network (AccNet)，将耗时在设计快速的算法变成训练AccNet的工作
  * 过程分为两步
    * 将splatting, blurring, slicing转为卷积操作
    * 将如上的卷积操作变成 *g* CP layers 去构造AccNet，当训练完成后，激活函数 *g* 和AccNet权重一起定义了新的splatting, blurring and slicing操作
# quantization
* [A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication]
  * 作者利用两种通信稀疏方案学习了 *convergence rate of distributed SGD for non-convex optimization*
    * *sparse parameter averaging*
    * *gradient quantization*
  * 作者提出 *PQASGD* 进一步减少通信量
* [GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training](https://arxiv.org/abs/1811.03617)
# distillation
* [Moonshine: Distilling with Cheap Convolutions](https://arxiv.org/abs/1711.02613)
* [Knowledge Distillation by On-the-Fly Native Ensemble](https://arxiv.org/abs/1806.04606)
* [KDGAN: Knowledge Distillation with Generative Adversarial Networks](http://aliensunmin.github.io/aii_workshop/2nd/slides/8.pdf)
 * ppt not paper
# Pruning
* [Frequency-Domain Dynamic Pruning for Convolutional Neural Networks]
 * MSRA
* [Discrimination-aware Channel Pruning for Deep Neural Networks](https://arxiv.org/abs/1810.11809)
* [An Efficient Pruning Algorithm for Robust Isotonic Regression]
