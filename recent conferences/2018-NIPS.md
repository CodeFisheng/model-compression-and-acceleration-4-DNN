# compression
* [Paraphrasing Complex Network: Network Compression via Factor Transfer]
* [Communication Compression for Decentralized Training]
  * 似乎发表了一个既能解决带宽要求，也能实现去中心化的系统。
  * 提出一个量化的且去中心化的训练过程，分为如下两个阶段
    * extrapolation compression
    * difference compression
  * highlight
    * quantized
    * decentralized
# acceleration
* [Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution]
  * 针对高维卷积耗时的问题，提出Acceleration Network (AccNet)，将耗时在设计快速的算法变成训练AccNet的工作
  * 过程分为两步
    * 将splatting, blurring, slicing转为卷积操作
    * 将如上的卷积操作变成 *g* CP layers 去构造AccNet，当训练完成后，激活函数 *g* 和AccNet权重一起定义了新的splatting, blurring and slicing操作
# quantization
* [A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication]
  * 作者利用两种通信稀疏方案学习了 *convergence rate of distributed SGD for non-convex optimization*
    * *sparse parameter averaging*
    * *gradient quantization*
  * 作者提出 *PQASGD* 进一步减少通信量
* [GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training]
