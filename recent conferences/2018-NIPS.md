# compression
* [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://arxiv.org/abs/1802.04977)
* [Communication Compression for Decentralized Training](https://arxiv.org/abs/1803.06443)
  * 似乎发表了一个既能解决带宽要求，也能实现去中心化的系统。
  * 提出一个量化的且去中心化的训练过程，分为如下两个阶段
    * extrapolation compression
    * difference compression
  * highlight
    * quantized
    * decentralized
# acceleration
* [Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution](https://dailongquan.github.io/files/publications/nips2018(1).pdf)
  * 针对高维卷积耗时的问题，提出Acceleration Network (AccNet)，将耗时在设计快速的算法变成训练AccNet的工作
  * 过程分为两步
    * 将splatting, blurring, slicing转为卷积操作
    * 将如上的卷积操作变成 *g* CP layers 去构造AccNet，当训练完成后，激活函数 *g* 和AccNet权重一起定义了新的splatting, blurring and slicing操作
# quantization
* [A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication]
  * 作者利用两种通信稀疏方案学习了 *convergence rate of distributed SGD for non-convex optimization*
    * *sparse parameter averaging*
    * *gradient quantization*
  * 作者提出 *PQASGD* 进一步减少通信量
* [GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training](https://arxiv.org/abs/1811.03617)
# distillation
* [Moonshine: Distilling with Cheap Convolutions](https://arxiv.org/abs/1711.02613)
* [Knowledge Distillation by On-the-Fly Native Ensemble](https://arxiv.org/abs/1806.04606)
* [KDGAN: Knowledge Distillation with Generative Adversarial Networks](http://aliensunmin.github.io/aii_workshop/2nd/slides/8.pdf)
  * ppt not paper
# Pruning
* [Frequency-Domain Dynamic Pruning for Convolutional Neural Networks]
  * MSRA
  * 提出在频率域进行网络系数的动态裁剪的方法，针对每次训练迭代和不同的频带，用动态的阈值来指导裁剪
  * 实验结果表明，频域动态裁剪显著优于传统的空域裁剪方法。特别是对于ResNet-110
  * [Compressing Convolutional Neural Networks in the Frequency Domain](https://www.kdd.org/kdd2016/papers/files/rpp0534-chenA.pdf)
* [Discrimination-aware Channel Pruning for Deep Neural Networks](https://arxiv.org/abs/1810.11809)
  * Tencent AI Lab *discrimination-aware channel pruning*
  * *discrimination-aware loss* and *reconstruction error*
  * 对于一个预训练好的网络剪枝，均匀的加入额外的loss。分成多阶段剪枝，每一阶段将所属层的loss，*reconstruction error*，最后的loss合起来做通道剪枝
  * 念头
    * 传统利用MSE来衡量 *reconstruction error* 的方法太依赖预训练模型的效果
    * 无法精确给中间层网络定性
  * 作者提出的算法分为如下操作
    * 从前往后遍历层，LBL剪枝，对于第i层建立 *construction error*
      * *construction error* 公式的大概意思是 累积当前层中所有channel中每一个类别对最终输出信息的贡献
      * 可变形加入交叉熵，y=ax+b
    * 然后将这个error和本身网络的loss进行反向传播
    * 挑出在反向中最敏感的通道（梯度更新最大的）加入集合A
    * 依据终止条件结束finetune
      * 手动选择个数
      * 矩阵的变动对于初始比例达到一个阈值
* [An Efficient Pruning Algorithm for Robust Isotonic Regression]
