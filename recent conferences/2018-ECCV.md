# model compression
* [Learning Compression from Limited Unlabeled Data](http://openaccess.thecvf.com/content_ECCV_2018/html/Xiangyu_He_Learning_Compression_from_ECCV_2018_paper.html)
  * *re-normalization* ，高斯共轭先验由batch normalization决定，导致了模型的退化。于是通过在batch normalization中运用 *re-estimated statistics* 
  * 使用 *l<sub>2</sub>* 范数去衡量量化误差（这种方法会对一些大数值敏感），于是加入一个 *scaling factor* 乘在量化矩阵中（值域由点变成线）。但是*l<sub>2</sub>* 很难在Q为离散空间限制下有 *global minima* ，展开*l<sub>2</sub>* 去求 *local minima* ，
  * **未看完**
* [Extreme Network Compression via Filter Group Approximation](http://cn.arxiv.org/abs/1807.11254)
  * 海康威视 decomposition method based on filter group approximation，作者focus于 the filter group structure for each layer
  * 
* [Compressing the Input for CNNs with the First-Order Scattering Transform](https://arxiv.org/abs/1809.10200?context=cs.LG)
* [AMC: AutoML for Model Compression and Acceleration on Mobile Devices](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.pdf)
* [Clustering Convolutional Kernels to Compress Deep Neural Networks](http://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Son_Clustering_Kernels_for_ECCV_2018_paper.pdf)
* [Constraint-Aware Deep Neural Network Compression](http://openaccess.thecvf.com/content_ECCV_2018/papers/Changan_Chen_Constraints_Matter_in_ECCV_2018_paper.pdf)
  * 文章的中心思想引入*constrained Basyesian optimization* 加入*cooling function*，引导逐步完成压缩目标（针对统一的压缩工作）
    * cooling function 分为 linear 和 exponential
      * linear: g<sub>t</sub>=p<sub>i</sub>(F<sub>0</sub>)+t/T\*(c<sub>i</sub>-p<sub>i</sub>(F<sub>0</sub>)),可见t=T时，完成最终的压缩
      * exponential： g<sub>t</sub>=c+e^(t/T)
    * 思想：前期的网络可能很适应去压缩，但是随着压缩的提升，很难再压缩
# quantization
* [LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks](http://cn.arxiv.org/abs/1807.10029)
  * 传统的手动量化很死板，灵活的方法不能改变方式的死板，所以需要训练一个量化器 train a QNN and its *quantizer* 量化器随着网络一起训练，基于 * quantization error minimization* 达到最好的效果，*权值*和*激活*皆量化，针对任意bit的量化，逐层且不共享权值
  * *quantization error minimization* 的公式意思为对（一个数的概率密度乘上这个数的量化误差的平方）求积分
  * 文章的量化为vector quantization的变形，*Q=v<sub>T</sub>e<subl</sub>* 这里的*v<sub>T</sub>* 指代这我们学习的量化的值，*e<subl</sub>* 是mask，值域{-1，1}或者{0，1}都可以。 所以也就是通过学习到的值和mask相乘得到量化后的值。文章的量化器的优化也就是优化*v<sub>T</sub>* 的值使得能够更好地去表达数据
  * *quantization error* 的公式是用*l<sub>2</sub>* 范式累加,此时在反向传播中有两个量需要优化，一个是mask一个是V<sub>T</sub>
  * 于是用*block coordinate descent*的方式，首先用上一次的V更新这次的B，再用这次的B更新这次的V
    * B是用2^k暴力找到loss最小的
    * V是用loss求(BB<sup>T</sup>)<sup>-1</sup>BX 最小
    * 每轮重复T次，通常设为1，因为作者实验中增大T没有明显作用
* [Product Quantization Network for Fast Image Retrieval](https://cse.buffalo.edu/~jsyuan/papers/2018/Product%20Quantization%20Network%20for%20Fast%20Image.pdf)
* [Value-aware Quantization for Training and Inference of Neural Networks](https://arxiv.org/abs/1804.07802)
  * *facebook and SNU*
* [Quantization Mimic: Towards Very Tiny CNN for Object Detection](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yi_Wei_Quantization_Mimic_Towards_ECCV_2018_paper.pdf)
* [LSQ++: Lower running time and higher recall in multi-codebook quantization](http://openaccess.thecvf.com/content_ECCV_2018/papers/Julieta_Martinez_LSQ_lower_runtime_ECCV_2018_paper.pdf)
# other
* [SaaS: Speed as a Supervisor for Semi-supervised Learning](http://openaccess.thecvf.com/content_ECCV_2018/papers/Safa_Cicek_SaaS_Speed_as_ECCV_2018_paper.pdf)
