# model compression
* [AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training](https://arxiv.org/abs/1712.02679)
* [Deep Neural Network Compression with Single and Multiple Level Quantization](https://arxiv.org/abs/1803.03289)
   * 本文主要思想是逐步完成量化
   * 首先*single level quantization*，也就是对单层量化。首先用*k-means*把weights划分为几个聚类，以每个聚类作为一个group，计算出每个group的loss（大概是作差）。把loss大的的clusters先量化，再对剩下的re-train，直到全部完成量化
      * 只能对相对高一点精度的量化（8-bit等），低精度（2-bit等）类数目少，loss大无法弥补
      * 用mask表示是否被量化，训练只更新未被量化的值，弥补量化带来的loss
      * loss组成为网络本身的loss和（scalar\*量化的误差)
   * *extend SLQ* 8-bit 量化划分为 8-centroids这种
   * *mutiple level quantization*
      * 考虑到层的作用是不一样的，layer-wise，其他如上
      * 针对低精度量化
# quantization
* [Adaptive Quantization for Deep Neural Network](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16248)
* [DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16652)
    * 本文却另辟蹊径，从非权重层入手来进行模型压缩。
    * 如ResNet、GoogLenet等的卷积层都是由很小的卷积核组成，本身就非常紧致了，并且也去掉了非常占参数量的全连接层。而Non-tensor layer（也就是非权重层，如pooling、BN、LRN、ReLU等等）反而成为了模型在cpu以及其他嵌入式硬件上达到real-time的最大阻碍
    * *streamline*
      * 将这一连串的层合并起来(Non-Tensor层（Pooling、LRN）),对于Pooling层，将stride直接乘到Conv层中
    * *branch merging*
      * 主要针对GoogLeNet中的Inception结构
      * 作者将比较细小的卷积层（1*1）以及Pooling层所在的分支，直接合并到和它并排的大卷积分支中
    * 对于一个预训练模型，作者逐层进行合并，合并得到的 *新层* 使用 *标准的初始化方式* ，*其他层的参数保留原预训练模型的参数*，然后将新层的学习率调高为其他层的10倍，进行finetuning，对于某些层，如GoogLenet中的Inception 4b-4d可以一起进行合并在finetuning。
* [Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM](https://arxiv.org/abs/1707.09870)
  * 待复现
# accleration
* [Learning a Wavelet-Like Auto-Encoder to Accelerate Deep Neural Networks](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16655)
