# model compression
* [AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training](https://arxiv.org/abs/1712.02679)
* [Deep Neural Network Compression with Single and Multiple Level Quantization](https://arxiv.org/abs/1803.03289)
# quantization
* [Adaptive Quantization for Deep Neural Network](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16248)
* [DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16652)
    * 本文却另辟蹊径，从非权重层入手来进行模型压缩。
    * 如ResNet、GoogLenet等的卷积层都是由很小的卷积核组成，本身就非常紧致了，并且也去掉了非常占参数量的全连接层。而Non-tensor layer（也就是非权重层，如pooling、BN、LRN、ReLU等等）反而成为了模型在cpu以及其他嵌入式硬件上达到real-time的最大阻碍
    * *streamline*
      * 将这一连串的层合并起来(Non-Tensor层（Pooling、LRN）),对于Pooling层，将stride直接乘到Conv层中
    * *branch merging*
      * 主要针对GoogLeNet中的Inception结构
      * 作者将比较细小的卷积层（1*1）以及Pooling层所在的分支，直接合并到和它并排的大卷积分支中
    * 对于一个预训练模型，作者逐层进行合并，合并得到的 *新层* 使用 *标准的初始化方式* ，*其他层的参数保留原预训练模型的参数*，然后将新层的学习率调高为其他层的10倍，进行finetuning，对于某些层，如GoogLenet中的Inception 4b-4d可以一起进行合并在finetuning。
* [Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM](https://arxiv.org/abs/1707.09870)
  * 待复现
# accleration
* [Learning a Wavelet-Like Auto-Encoder to Accelerate Deep Neural Networks](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16655)
