# decomposition
* [Learning Compact Recurrent Neural Networks With Block-Term Tensor Decomposition](https://arxiv.org/abs/1712.05134)
* [Wide Compression: Tensor Ring Nets](https://arxiv.org/abs/1802.09052)
# distillation
* [Fast and Accurate Single Image Super-Resolution via Information Distillation Network](https://arxiv.org/abs/1803.09454)
* [PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing](https://arxiv.org/abs/1805.04409)
# pruning
* [CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization](http://www.sfu.ca/~ftung/papers/clipq_cvpr18.pdf)
* [NISP: Pruning Networks using Neuron Importance Score Propagation](https://arxiv.org/abs/1711.05908)
# quantization
* [learning-Compression Algorithms for Neural Net Pruning](http://openaccess.thecvf.com/content_cvpr_2018/html/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.html)
* [A Biresolution Spectral Framework for Product Quantization](http://openaccess.thecvf.com/content_cvpr_2018/html/1103.html)
* [CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization](http://www.sfu.ca/~ftung/papers/clipq_cvpr18.pdf)
* [Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks](https://ai.intel.com/nervana/wp-content/uploads/sites/53/2018/06/ELQ_CameraReady_CVPR2018.pdf)
* [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf)
* [SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks](https://arxiv.org/abs/1807.00301)
* [Model compression via distillation and quantization ](https://arxiv.org/abs/1802.05668v1)
  * [解读](https://blog.csdn.net/yingpeng_zhong/article/details/80213016)
* [Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation](http://cn.arxiv.org/abs/1803.04907)
  * 利用量化减少过拟提高精度
  * [解读..](https://baijiahao.baidu.com/s?id=1596600100072005733&wfr=spider&for=pc)
* [Two-Step Quantization for Low-bit Neural Networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Two-Step_Quantization_for_CVPR_2018_paper.pdf)
  * [解读](https://blog.csdn.net/qq_19784349/article/details/83931420)
  * 文章的2step指先量化激活值再量化权重 *code learning step* and *transformation function learning step*
    * 对于激活值的量化，作者提出 *sparse quantization for code learning*，只有权重被量化。设定阈值，小于阈值的置零；大于阈值的值按照 2^n(n: bit)个区间来量化。
      * 经过BN层的激活值基本处于高斯分布，再经过ReLU层，则可以看做是大概产生了50%的稀疏。另一方面，是由于 attention 机制，数值较大的激活值比较小的激活值更重要。
      * 量化有一个浮点系数
    * 对于权重量化的问题，*非线性最小二乘法问题* 。
      * 由于存在激活值量化和weight量化，为了把两个过程分开，会生成一个新的公式。now<sub>err</sub>=Q(x)<sub>err</sub>+k*quantization<sub>err</sub>
  * 结果
    * AlexNet on imagenet top1 drop 0.5%  and top5 drop 1%
    * VGG-16 on imagenet top1 drop 2.0% and top5 drop 0.7%
