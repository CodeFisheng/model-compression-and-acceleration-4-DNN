# model compression
* [Compressing Neural Networks using the Variational Information Bottleneck](https://arxiv.org/abs/1802.10399?context=cs.CV)
* [Weightless: Lossy Weight Encoding For Deep Neural Network Compression](https://arxiv.org/abs/1711.04686?context=stat.ML)
* [Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions](http://cn.arxiv.org/abs/1806.09228)
* [Understanding Generalization and Optimization Performance of Deep CNNs](https://arxiv.org/abs/1805.10767v1)
* [Stronger Generalization Bounds for Deep Nets via a Compression Approach](http://proceedings.mlr.press/v80/arora18b.html)
* [On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization](http://proceedings.mlr.press/v80/arora18a.html)
* [Exploring Hidden Dimensions in Accelerating Convolutional Neural Networks](http://proceedings.mlr.press/v80/jia18a.html)
# distillation
* [Adversarial Distillation of Bayesian Neural Network Posteriors](http://proceedings.mlr.press/v80/wang18i.html)
